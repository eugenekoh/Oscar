{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cutting-sacrifice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "os.chdir('../datasets/personality_captions')\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-visitor",
   "metadata": {},
   "source": [
    "# Clean Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ordinary-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    return Counter(d['comment'] for d in data)\n",
    "counts = {x : load_json(f'{x}.json') for x in ['train', 'val', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "operating-malaysia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [('[DISCONNECT]', 68), ('[RETURNED]', 28), ('What is this?', 15), ('[TIMEOUT]', 13), ('What is that?', 12), ('Where is this?', 11), ('What is going on here?', 10), ('This looks like so much fun!', 9), (\"What's going on here?\", 9), ('what is that?', 7)] \n",
      "\n",
      "val [('[DISCONNECT]', 5), ('I would hide', 2), ('THE GUY IS SUFFERING FROM DEPRESSION', 1), ('Is she doing a backflip twist?', 1), ('What did they have to do to help support cancer? Fundraisers always intrigue me', 1), ('I love the bit of motion blur in this photo, asks plenty of questions as to why this person took such an active, lovely photo.', 1), ('Sunlight filtered by sleepy clouds.', 1), ('what a wonderful map', 1), ('Why is that guy bulling the little one.', 1), (\"She's really cool and funny\", 1)] \n",
      "\n",
      "test [('[DISCONNECT]', 10), ('[RETURNED]', 3), ('[TIMEOUT]', 3), ('I do not have time for this.', 2), ('What is going on here?', 2), ('This is the life', 2), ('What even is this?', 2), ('I love the way this color makes me feel', 2), (\"I can't decide.\", 2), ('i love this country', 2)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in counts.items():\n",
    "    print(k, v.most_common(10), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "usual-fleece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning train.json, before : 186858\n",
      "after : 186749\n",
      "cleaning val.json, before : 5000\n",
      "after : 4994\n",
      "cleaning test.json, before : 10000\n",
      "after : 9984\n"
     ]
    }
   ],
   "source": [
    "# clean the captions data with [DISCONNECT], [RETURNED] and [TIMEOUT].\n",
    "def clean_captions(file):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    tokens = ['[DISCONNECT]', '[RETURNED]', '[TIMEOUT]']\n",
    "    print(f'cleaning {file}, before : {len(data)}')\n",
    "    \n",
    "    cleaned = [d for d in data if d['comment'] not in tokens]\n",
    "    \n",
    "    print(f'after : {len(cleaned)}')\n",
    "    with open(file, 'w') as f:\n",
    "        json.dump(cleaned, f)\n",
    "\n",
    "for x in ['train', 'val', 'test']:\n",
    "    clean_captions(f'{x}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "similar-collect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [('What is this?', 15), ('What is that?', 12), ('Where is this?', 11), ('What is going on here?', 10), ('This looks like so much fun!', 9), (\"What's going on here?\", 9), ('what is that?', 7), ('What is he doing?', 7), ('what is this', 6), ('Is this supposed to be art?', 6)] \n",
      "\n",
      "len 1 [] \n",
      "\n",
      "val [('I would hide', 2), ('THE GUY IS SUFFERING FROM DEPRESSION', 1), ('Is she doing a backflip twist?', 1), ('What did they have to do to help support cancer? Fundraisers always intrigue me', 1), ('I love the bit of motion blur in this photo, asks plenty of questions as to why this person took such an active, lovely photo.', 1), ('Sunlight filtered by sleepy clouds.', 1), ('what a wonderful map', 1), ('Why is that guy bulling the little one.', 1), (\"She's really cool and funny\", 1), ('These people sitting at nice tables could have dressed a bit nicer, too.', 1)] \n",
      "\n",
      "len 1 [] \n",
      "\n",
      "test [('I do not have time for this.', 2), ('What is going on here?', 2), ('This is the life', 2), ('What even is this?', 2), ('I love the way this color makes me feel', 2), (\"I can't decide.\", 2), ('i love this country', 2), ('I wish I was there!', 2), (\"A little heavy on the make-up don't ya think. \", 1), ('Something about the pattern calms me', 1)] \n",
      "\n",
      "len 1 [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "counts = {x : load_json(f'{x}.json') for x in ['train', 'val', 'test']}\n",
    "for k, v in counts.items():\n",
    "    print(k, v.most_common(10), '\\n')\n",
    "    print('len 1', [x for x in v.keys() if len(x.split(' ')) == 1], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "usual-notebook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total captions : 201727\n"
     ]
    }
   ],
   "source": [
    "print(f'total captions : {sum(sum(v.values()) for v in counts.values())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-springfield",
   "metadata": {},
   "source": [
    "# Split TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "engaged-dodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "856it [00:00, 8558.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 186749, 'val': 4994, 'test': 9984}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201230it [00:22, 8767.67it/s]\n",
      "8it [00:00, 75.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 186238, 'val': 4989, 'test': 9977}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201230it [31:24, 106.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 186238, 'val': 4989, 'test': 9977}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_json(file):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    return set(x['image_hash'] for x in data)\n",
    "\n",
    "    \n",
    "def split(infile, ids):\n",
    "    # merge tsv files\n",
    "    with tqdm() as pbar:\n",
    "        infile = Path(infile)\n",
    "        train_name = infile.parent / f\"train.{infile.name}\"\n",
    "        test_name = infile.parent / f\"test.{infile.name}\"\n",
    "        val_name = infile.parent / f\"val.{infile.name}\"\n",
    "        \n",
    "        counts = {k : 0 for k in ids.keys()}\n",
    "        \n",
    "        with open(train_name, 'w') as train_tsv, open(test_name, 'w') as test_tsv, open(val_name, 'w') as val_tsv, open(\"/dev/null\", 'w') as dummy:\n",
    "            \n",
    "            train_writer = csv.writer(train_tsv, delimiter = '\\t')   \n",
    "            test_writer = csv.writer(test_tsv, delimiter = '\\t')   \n",
    "            val_writer = csv.writer(val_tsv, delimiter = '\\t')   \n",
    "            \n",
    "            dummy_writer = csv.writer(dummy, delimiter = '\\t')\n",
    "            \n",
    "            \n",
    "            with open(infile) as in_tsv:\n",
    "                reader = csv.reader(in_tsv, delimiter='\\t')\n",
    "                \n",
    "                for item in reader:\n",
    "                    image_id = item[0]\n",
    "                    \n",
    "                    # check for write errors\n",
    "                    try:\n",
    "                        dummy_writer.writerow(item)\n",
    "                    except Exception as e:\n",
    "                        tqdm.write(f'write error for {image_id}, {str(e)}')\n",
    "                        continue\n",
    "                        \n",
    "                    if image_id in ids['train']:\n",
    "                        train_writer.writerow(item)\n",
    "                        counts['train'] += 1\n",
    "                    elif image_id in ids['test']:\n",
    "                        test_writer.writerow(item)\n",
    "                        counts['test'] += 1\n",
    "                    elif image_id in ids['val']:\n",
    "                        val_writer.writerow(item)\n",
    "                        counts['val'] += 1\n",
    "\n",
    "                    pbar.update(1)\n",
    "    tqdm.write(str(counts))\n",
    "ids = {x : load_json(f'{x}.json') for x in ['train', 'val', 'test']}\n",
    "tqdm.write(str({k : len(v) for k, v in ids.items()}))\n",
    "\n",
    "split('label.tsv', ids)\n",
    "split('feature.tsv', ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-fighter",
   "metadata": {},
   "source": [
    "# Validate Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "oriented-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(feat_file, label_file):\n",
    "    def get_ids(f):\n",
    "         with open(f) as in_tsv:\n",
    "            reader = csv.reader(in_tsv, delimiter='\\t')\n",
    "            ids = {}\n",
    "            for i, x in enumerate(reader):\n",
    "                if x[0] in ids:\n",
    "                    print(f'duplicate {x[0]}')\n",
    "                ids[x[0]] = i \n",
    "            return ids\n",
    "        \n",
    "    feat_ids = get_ids(feat_file)\n",
    "    label_ids = get_ids(label_file)\n",
    "    \n",
    "    assert len(feat_ids) == len(label_ids)\n",
    "    assert feat_ids.keys() == label_ids.keys()\n",
    "    assert all(idx == label_ids[k] for k, idx in feat_ids.items())\n",
    "    print(len(label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "lovely-affairs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "186238\n",
      "val\n",
      "4989\n",
      "test\n",
      "9977\n"
     ]
    }
   ],
   "source": [
    "for x in ['train', 'val', 'test']:\n",
    "    print(x)\n",
    "    validate(f'{x}.feature.tsv',f'{x}.label.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-titanium",
   "metadata": {},
   "source": [
    "# Lineidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "smooth-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "def generate_lineidx(filein, idxout):\n",
    "    idxout_tmp = idxout + '.tmp'\n",
    "    with open(filein, 'r') as tsvin, open(idxout_tmp,'w') as tsvout:\n",
    "        fsize = os.fstat(tsvin.fileno()).st_size\n",
    "        fpos = 0\n",
    "        while fpos!=fsize:\n",
    "            tsvout.write(str(fpos)+\"\\n\")\n",
    "            tsvin.readline()\n",
    "            fpos = tsvin.tell()\n",
    "    os.rename(idxout_tmp, idxout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "solid-potential",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "val\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for x in ['train', 'val', 'test']:\n",
    "    print(x)\n",
    "    generate_lineidx(f'{x}.feature.tsv', f'{x}.feature.lineidx')\n",
    "    generate_lineidx(f'{x}.label.tsv', f'{x}.label.lineidx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-webster",
   "metadata": {},
   "source": [
    "# Remove missing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "mathematical-premiere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "186238\n",
      "val\n",
      "4989\n",
      "test\n",
      "9977\n"
     ]
    }
   ],
   "source": [
    "def update_captions(label_tsv, caption_json):\n",
    "    with open(label_tsv) as in_tsv:\n",
    "        reader = csv.reader(in_tsv, delimiter='\\t')\n",
    "        label_ids = {x[0] for x in reader}\n",
    "        \n",
    "    with open(caption_json) as f:\n",
    "        captions = json.load(f)\n",
    "    \n",
    "    captions = [x for x in captions if x['image_hash'] in label_ids]\n",
    "    \n",
    "    assert len(captions) == len(label_ids)\n",
    "    \n",
    "    with open(caption_json, 'w') as f:\n",
    "        json.dump(captions, f)\n",
    "    \n",
    "    print(len(captions))\n",
    "for x in ['train', 'val', 'test']:\n",
    "    print(x)\n",
    "    update_captions(f'{x}.label.tsv', f'{x}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-short",
   "metadata": {},
   "source": [
    "# Update Personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sustained-exclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning train.json, before : 186238\n",
      "after : 186238\n",
      "cleaning val.json, before : 4989\n",
      "after : 4989\n",
      "cleaning test.json, before : 9977\n",
      "after : 9977\n"
     ]
    }
   ],
   "source": [
    "# remove it to one word\n",
    "personalities = set()\n",
    "def clean_personality(file):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    print(f'cleaning {file}, before : {len(data)}')\n",
    "    \n",
    "    cleaned = []\n",
    "    for d in data:\n",
    "        d['personality'] = d['personality'].split(' ')[0]\n",
    "        personalities.add(d['personality'])\n",
    "        cleaned.append(d)\n",
    "        \n",
    "    print(f'after : {len(cleaned)}')\n",
    "    \n",
    "    with open(file, 'w') as f:\n",
    "        json.dump(cleaned, f)\n",
    "\n",
    "for x in ['train', 'val', 'test']:\n",
    "    clean_personality(f'{x}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "colored-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(x) == 1 for x in personalities)\n",
    "\n",
    "with open('personalities.txt', 'w') as f:\n",
    "    f.writelines(str(x) + '\\n' for x in personalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-light",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
