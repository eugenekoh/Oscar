{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "industrial-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "os.chdir('/home/eugene/Documents/workspace/Oscar/datasets/personality_captions')\n",
    "#os.chdir('/home/eugene/anaconda3/envs/frcnn/lib/python3.7/site-packages/data/personality_captions')\n",
    "_ = csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-assist",
   "metadata": {},
   "source": [
    "# Clean Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "parliamentary-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    return Counter(d['comment'] for d in data)\n",
    "counts = {x : load_json(f'{x}.json') for x in ['train', 'val', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "straight-shock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [('[DISCONNECT]', 68), ('[RETURNED]', 28), ('What is this?', 15), ('[TIMEOUT]', 13), ('What is that?', 12), ('Where is this?', 11), ('What is going on here?', 10), ('This looks like so much fun!', 9), (\"What's going on here?\", 9), ('what is that?', 7)] \n",
      "\n",
      "val [('[DISCONNECT]', 5), ('I would hide', 2), ('THE GUY IS SUFFERING FROM DEPRESSION', 1), ('Is she doing a backflip twist?', 1), ('What did they have to do to help support cancer? Fundraisers always intrigue me', 1), ('I love the bit of motion blur in this photo, asks plenty of questions as to why this person took such an active, lovely photo.', 1), ('Sunlight filtered by sleepy clouds.', 1), ('what a wonderful map', 1), ('Why is that guy bulling the little one.', 1), (\"She's really cool and funny\", 1)] \n",
      "\n",
      "test [('[DISCONNECT]', 10), ('[RETURNED]', 3), ('[TIMEOUT]', 3), ('I do not have time for this.', 2), ('What is going on here?', 2), ('This is the life', 2), ('What even is this?', 2), ('I love the way this color makes me feel', 2), (\"I can't decide.\", 2), ('i love this country', 2)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in counts.items():\n",
    "    print(k, v.most_common(10), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "nonprofit-praise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning train.json, before : 186858\n",
      "after : 186749\n",
      "cleaning val.json, before : 5000\n",
      "after : 4994\n",
      "cleaning test.json, before : 10000\n",
      "after : 9984\n"
     ]
    }
   ],
   "source": [
    "# clean the captions data with [DISCONNECT], [RETURNED] and [TIMEOUT].\n",
    "def clean_captions(file):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    tokens = ['[DISCONNECT]', '[RETURNED]', '[TIMEOUT]']\n",
    "    print(f'cleaning {file}, before : {len(data)}')\n",
    "    \n",
    "    cleaned = [d for d in data if d['comment'] not in tokens]\n",
    "    \n",
    "    print(f'after : {len(cleaned)}')\n",
    "    with open(file, 'w') as f:\n",
    "        json.dump(cleaned, f)\n",
    "\n",
    "for x in ['train', 'val', 'test']:\n",
    "    clean_captions(f'{x}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "molecular-thailand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [('What is this?', 15), ('What is that?', 12), ('Where is this?', 11), ('What is going on here?', 10), ('This looks like so much fun!', 9), (\"What's going on here?\", 9), ('what is that?', 7), ('What is he doing?', 7), ('what is this', 6), ('Is this supposed to be art?', 6)] \n",
      "\n",
      "len 1 [] \n",
      "\n",
      "val [('I would hide', 2), ('THE GUY IS SUFFERING FROM DEPRESSION', 1), ('Is she doing a backflip twist?', 1), ('What did they have to do to help support cancer? Fundraisers always intrigue me', 1), ('I love the bit of motion blur in this photo, asks plenty of questions as to why this person took such an active, lovely photo.', 1), ('Sunlight filtered by sleepy clouds.', 1), ('what a wonderful map', 1), ('Why is that guy bulling the little one.', 1), (\"She's really cool and funny\", 1), ('These people sitting at nice tables could have dressed a bit nicer, too.', 1)] \n",
      "\n",
      "len 1 [] \n",
      "\n",
      "test [('I do not have time for this.', 2), ('What is going on here?', 2), ('This is the life', 2), ('What even is this?', 2), ('I love the way this color makes me feel', 2), (\"I can't decide.\", 2), ('i love this country', 2), ('I wish I was there!', 2), (\"A little heavy on the make-up don't ya think. \", 1), ('Something about the pattern calms me', 1)] \n",
      "\n",
      "len 1 [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "counts = {x : load_json(f'{x}.json') for x in ['train', 'val', 'test']}\n",
    "for k, v in counts.items():\n",
    "    print(k, v.most_common(10), '\\n')\n",
    "    print('len 1', [x for x in v.keys() if len(x.split(' ')) == 1], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pharmaceutical-paragraph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total captions : 201727\n"
     ]
    }
   ],
   "source": [
    "print(f'total captions : {sum(sum(v.values()) for v in counts.values())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-taxation",
   "metadata": {},
   "source": [
    "# Split TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "complicated-inquiry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "856it [00:00, 8558.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 186749, 'val': 4994, 'test': 9984}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201230it [00:22, 8767.67it/s]\n",
      "8it [00:00, 75.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 186238, 'val': 4989, 'test': 9977}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201230it [31:24, 106.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 186238, 'val': 4989, 'test': 9977}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_json(file):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    return set(x['image_hash'] for x in data)\n",
    "\n",
    "    \n",
    "def split(infile, ids):\n",
    "    # merge tsv files\n",
    "    with tqdm() as pbar:\n",
    "        infile = Path(infile)\n",
    "        train_name = infile.parent / f\"train.{infile.name}\"\n",
    "        test_name = infile.parent / f\"test.{infile.name}\"\n",
    "        val_name = infile.parent / f\"val.{infile.name}\"\n",
    "        \n",
    "        counts = {k : 0 for k in ids.keys()}\n",
    "        \n",
    "        with open(train_name, 'w') as train_tsv, open(test_name, 'w') as test_tsv, open(val_name, 'w') as val_tsv, open(\"/dev/null\", 'w') as dummy:\n",
    "            \n",
    "            train_writer = csv.writer(train_tsv, delimiter = '\\t')   \n",
    "            test_writer = csv.writer(test_tsv, delimiter = '\\t')   \n",
    "            val_writer = csv.writer(val_tsv, delimiter = '\\t')   \n",
    "            \n",
    "            dummy_writer = csv.writer(dummy, delimiter = '\\t')\n",
    "            \n",
    "            \n",
    "            with open(infile) as in_tsv:\n",
    "                reader = csv.reader(in_tsv, delimiter='\\t')\n",
    "                \n",
    "                for item in reader:\n",
    "                    image_id = item[0]\n",
    "                    \n",
    "                    # check for write errors\n",
    "                    try:\n",
    "                        dummy_writer.writerow(item)\n",
    "                    except Exception as e:\n",
    "                        tqdm.write(f'write error for {image_id}, {str(e)}')\n",
    "                        continue\n",
    "                        \n",
    "                    if image_id in ids['train']:\n",
    "                        train_writer.writerow(item)\n",
    "                        counts['train'] += 1\n",
    "                    elif image_id in ids['test']:\n",
    "                        test_writer.writerow(item)\n",
    "                        counts['test'] += 1\n",
    "                    elif image_id in ids['val']:\n",
    "                        val_writer.writerow(item)\n",
    "                        counts['val'] += 1\n",
    "\n",
    "                    pbar.update(1)\n",
    "    tqdm.write(str(counts))\n",
    "ids = {x : load_json(f'{x}.json') for x in ['train', 'val', 'test']}\n",
    "tqdm.write(str({k : len(v) for k, v in ids.items()}))\n",
    "\n",
    "split('label.tsv', ids)\n",
    "split('feature.tsv', ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-pregnancy",
   "metadata": {},
   "source": [
    "# Validate Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "controlled-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(feat_file, label_file):\n",
    "    def get_ids(f):\n",
    "         with open(f) as in_tsv:\n",
    "            reader = csv.reader(in_tsv, delimiter='\\t')\n",
    "            ids = {}\n",
    "            for i, x in enumerate(reader):\n",
    "                if x[0] in ids:\n",
    "                    print(f'duplicate {x[0]}')\n",
    "                ids[x[0]] = i \n",
    "            return ids\n",
    "        \n",
    "    feat_ids = get_ids(feat_file)\n",
    "    label_ids = get_ids(label_file)\n",
    "    \n",
    "    assert len(feat_ids) == len(label_ids)\n",
    "    assert feat_ids.keys() == label_ids.keys()\n",
    "    assert all(idx == label_ids[k] for k, idx in feat_ids.items())\n",
    "    print(len(label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "affecting-moldova",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "186238\n",
      "val\n",
      "4989\n",
      "test\n",
      "9977\n"
     ]
    }
   ],
   "source": [
    "for x in ['train', 'val', 'test']:\n",
    "    print(x)\n",
    "    validate(f'{x}.feature.tsv',f'{x}.label.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-photography",
   "metadata": {},
   "source": [
    "# Lineidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fiscal-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "def generate_lineidx(filein, idxout):\n",
    "    idxout_tmp = idxout + '.tmp'\n",
    "    with open(filein, 'r') as tsvin, open(idxout_tmp,'w') as tsvout:\n",
    "        fsize = os.fstat(tsvin.fileno()).st_size\n",
    "        fpos = 0\n",
    "        while fpos!=fsize:\n",
    "            tsvout.write(str(fpos)+\"\\n\")\n",
    "            tsvin.readline()\n",
    "            fpos = tsvin.tell()\n",
    "    os.rename(idxout_tmp, idxout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "rational-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    " generate_lineidx(f'train.feature.tsv', f'train.feature.lineidx')\n",
    " generate_lineidx(f'train.label.tsv', f'train.label.lineidx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "congressional-healthcare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "val\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for x in ['train', 'val', 'test']:\n",
    "    print(x)\n",
    "    generate_lineidx(f'{x}.feature.tsv', f'{x}.feature.lineidx')\n",
    "    generate_lineidx(f'{x}.label.tsv', f'{x}.label.lineidx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-toner",
   "metadata": {},
   "source": [
    "# Remove missing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "upper-insertion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-45973d7fd0da>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'train'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'val'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'test'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m     \u001B[0mupdate_captions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'{x}.label.tsv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mf'{x}.json'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-25-45973d7fd0da>\u001B[0m in \u001B[0;36mupdate_captions\u001B[0;34m(label_tsv, caption_json)\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0mcaptions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mx\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcaptions\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'image_hash'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlabel_ids\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m     \u001B[0;32massert\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcaptions\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabel_ids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcaption_json\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'w'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def update_captions(label_tsv, caption_json):\n",
    "    with open(label_tsv) as in_tsv:\n",
    "        reader = csv.reader(in_tsv, delimiter='\\t')\n",
    "        label_ids = {x[0] for x in reader}\n",
    "        \n",
    "    with open(caption_json) as f:\n",
    "        captions = json.load(f)\n",
    "    \n",
    "    captions = [x for x in captions if x['image_hash'] in label_ids]\n",
    "    \n",
    "    assert len(captions) == len(label_ids)\n",
    "    \n",
    "    with open(caption_json, 'w') as f:\n",
    "        json.dump(captions, f)\n",
    "    \n",
    "    print(len(captions))\n",
    "for x in ['train', 'val', 'test']:\n",
    "    print(x)\n",
    "    update_captions(f'{x}.label.tsv', f'{x}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-friendship",
   "metadata": {},
   "source": [
    "# Update Personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "medical-matthew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning train.json, before : 186234\n",
      "after : 186234\n",
      "cleaning val.json, before : 4989\n",
      "after : 4989\n",
      "cleaning test.json, before : 9977\n",
      "after : 9977\n"
     ]
    }
   ],
   "source": [
    "# remove it to one word\n",
    "personalities = set()\n",
    "def clean_personality(file):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    print(f'cleaning {file}, before : {len(data)}')\n",
    "    \n",
    "    cleaned = []\n",
    "    for d in data:\n",
    "        d['personality'] = d['personality'].split(' ')[0]\n",
    "        personalities.add(d['personality'])\n",
    "        cleaned.append(d)\n",
    "        \n",
    "    print(f'after : {len(cleaned)}')\n",
    "    \n",
    "    with open(file, 'w') as f:\n",
    "        json.dump(cleaned, f)\n",
    "\n",
    "for x in ['train', 'val', 'test']:\n",
    "    clean_personality(f'{x}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "spatial-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(x) == 1 for x in personalities)\n",
    "\n",
    "with open('personalities.txt', 'w') as f:\n",
    "    f.writelines(str(x) + '\\n' for x in personalities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-daisy",
   "metadata": {},
   "source": [
    "# Remove 'Crude' Personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chubby-probe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning train.json, before : 186234\n",
      "after : 186234\n",
      "cleaning val.json, before : 4989\n",
      "after : 4989\n",
      "cleaning test.json, before : 9977\n",
      "after : 9977\n"
     ]
    }
   ],
   "source": [
    "# clean the captions data with 'Crude'\n",
    "def clean_captions(file):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    print(f'cleaning {file}, before : {len(data)}')\n",
    "    \n",
    "    cleaned = [d for d in data if d['personality'] != 'Crude' ]\n",
    "    \n",
    "    print(f'after : {len(cleaned)}')\n",
    "    with open(file, 'w') as f:\n",
    "        json.dump(cleaned, f)\n",
    "\n",
    "for x in ['train', 'val', 'test']:\n",
    "    clean_captions(f'{x}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-quick",
   "metadata": {},
   "source": [
    "## Get the distribution of personalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "earlier-brazil",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201200it [00:00, 403544.54it/s]\n"
     ]
    }
   ],
   "source": [
    "personalities = defaultdict(int)\n",
    "with tqdm() as pbar:\n",
    "    for i in ['train.json', 'val.json', 'test.json']:\n",
    "        with open(i) as f:\n",
    "            json_data = json.load(f)\n",
    "        for x in json_data:\n",
    "            personalities[x['personality']] += 1\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-aging",
   "metadata": {},
   "source": [
    "## update train label and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "armed-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_train(infile):\n",
    "    with open('train.json') as f:\n",
    "        ids = set(x['image_hash'] for x in json.load(f))\n",
    "\n",
    "    counts = 0\n",
    "    with open(infile+'.tmp', 'w') as train_tsv:\n",
    "        train_writer = csv.writer(train_tsv, delimiter = '\\t') \n",
    "        \n",
    "        with open(infile) as in_tsv:\n",
    "            reader = csv.reader(in_tsv, delimiter='\\t')\n",
    "\n",
    "            for item in tqdm(reader):\n",
    "                image_id = item[0]\n",
    "                \n",
    "                if image_id in ids:\n",
    "                    train_writer.writerow(item)\n",
    "                    counts += 1\n",
    "                    \n",
    "    os.rename(infile+'.tmp', infile)\n",
    "    tqdm.write(str(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "accessory-permission",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186238it [00:12, 15336.39it/s]\n",
      "186238it [17:08, 181.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186234\n",
      "186234\n"
     ]
    }
   ],
   "source": [
    "update_train('train.label.tsv')\n",
    "update_train('train.feature.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensure correct formatting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c07eb125b83823121ab72d7c2893c1c\t\"[{\"\"conf\"\": 0.37365537881851196, \"\"class\"\": \"\"sky\"\", \"\"rect\"\": [0.0, 0.0, 369.6669616699219, 167.16232299804688]}, {\"\"conf\"\": 0.27570459246635437, \"\"class\"\": \"\"branch\"\", \"\"rect\"\": [238.2344970703125, 0.0, 374.375, 325.0687255859375]}, {\"\"conf\"\": 0.2762376666069031, \"\"class\"\": \"\"branch\"\", \"\"rect\"\": [6.283855438232422, 353.80535888671875, 374.375, 499.375]}, {\"\"conf\"\": 0.28178179264068604, \"\"class\"\": \"\"tree\"\", \"\"rect\"\": [277.7760009765625, 0.0, 374.375, 278.257568359375]}, {\"\"conf\"\": 0.3297758400440216, \"\"class\"\": \"\"tree\"\", \"\"rect\"\": [218.90582275390625, 105.63359832763672, 374.375, 499.375]}, {\"\"conf\"\": 0.7046377062797546, \"\"class\"\": \"\"tree\"\", \"\"rect\"\": [64.18987274169922, 49.167537689208984, 320.56475830078125, 472.7288818359375]}, {\"\"conf\"\": 0.3796304762363434, \"\"class\"\": \"\"branch\"\", \"\"rect\"\": [19.837989807128906, 26.823034286499023, 196.2734832763672, 211.62310791015625]}, {\"\"conf\"\": 0.6425620913505554, \"\"class\"\": \"\"branch\"\", \"\"rect\"\": [13.209705352783203, 301.38775634765625, 303.3202209472656, 420.13287353515625]}, {\"\"conf\"\": 0.4831509590148926, \"\"class\"\": \"\"branch\"\", \"\"rect\"\": [318.1078186035156, 357.8233337402344, 374.375, 395.9016418457031]}, {\"\"conf\"\": 0.23258528113365173, \"\"class\"\": \"\"branch\"\", \"\"rect\"\": [320.2078857421875, 347.1374206542969, 374.375, 408.409912109375]}, {\"\"conf\"\": 0.5507535934448242, \"\"class\"\": \"\"branch\"\", \"\"rect\"\": [0.0, 229.8233184814453, 280.705322265625, 358.6988830566406]}, {\"\"conf\"\": 0.455393522977829, \"\"class\"\": \"\"branch\"\", \"\"rect\"\": [247.90109252929688, 317.0042419433594, 374.375, 481.71759033203125]}, {\"\"conf\"\": 0.23709195852279663, \"\"class\"\": \"\"branch\"\", \"\"rect\"\": [127.05489349365234, 0.0, 310.340087890625, 195.0]}, {\"\"conf\"\": 0.6320849657058716, \"\"class\"\": \"\"branch\"\", \"\"rect\"\": [197.53639221191406, 452.0385437011719, 334.3408203125, 499.375]}]\"\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# for train label\n",
    "head -n1 train.label.tsv\n",
    "sed -i 's/\"\"/\"/g' train.label.tsv\n",
    "sed -i 's/\"\\[/\\[/g' train.label.tsv\n",
    "sed -i 's/\\]\"/\\]/g' train.label.tsv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186234it [01:29, 2077.10it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('train.label.tsv') as in_tsv:\n",
    "    reader = csv.reader(in_tsv, delimiter='\\t')\n",
    "    for item in tqdm(reader):\n",
    "        x = ast.literal_eval(item[1])\n",
    "        assert type(x) == list\n",
    "        assert \"conf\"  in x[0]\n",
    "        assert \"class\" in x[0]\n",
    "        assert \"rect\" in x[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186234it [10:08, 306.22it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('train.feature.tsv') as in_tsv:\n",
    "    reader = csv.reader(in_tsv, delimiter='\\t')\n",
    "    for x in tqdm(reader):\n",
    "        feat_info = ast.literal_eval(x[1])\n",
    "        num_boxes = feat_info['num_boxes']\n",
    "        np_arr = np.frombuffer(base64.b64decode(feat_info['features']), np.float32).reshape((num_boxes, -1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "nominated-generic",
   "metadata": {},
   "source": [
    "# Remove the candidates from val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "loving-batch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning train.json, before : 186234\n",
      "after : 186234\n",
      "cleaning val.json, before : 4989\n",
      "after : 4989\n",
      "cleaning test.json, before : 9977\n",
      "after : 9977\n"
     ]
    }
   ],
   "source": [
    "def clean_captions(file):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    print(f'cleaning {file}, before : {len(data)}')\n",
    "    \n",
    "    for d in data:\n",
    "        for remove in ['500_candidates', 'candidates']:\n",
    "            if remove in d:\n",
    "                d.pop(remove)\n",
    "    \n",
    "    print(f'after : {len(data)}')\n",
    "    with open(file, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "for x in ['train', 'val', 'test']:\n",
    "    clean_captions(f'{x}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "lucky-swaziland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'personality': 'Obsessive', 'comment': \"A little heavy on the make-up don't ya think. \", 'additional_comments': ['she girl  is good', 'I just love her hair that I would keep it.', 'She needs to put some rings on, look, no rings, she needs more rings', 'She spent 3 hours on her look'], 'image_hash': '2923e28b6f588aff2d469ab2cccfac57'}\n"
     ]
    }
   ],
   "source": [
    "with open('test.json') as f:\n",
    "    data = json.load(f)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-calibration",
   "metadata": {},
   "source": [
    "# Add additional comments to test_caption_coco_format.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vocational-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "i = 0 \n",
    "images = []\n",
    "annotations = []\n",
    "for d in data:\n",
    "    images.append({\n",
    "        'id' : d['image_hash'], \n",
    "        'file_name' : d['image_hash']\n",
    "    })\n",
    "    \n",
    "    caps = d['additional_comments']\n",
    "    caps.append(d['comment'])\n",
    "    \n",
    "    for c in caps:\n",
    "        annotations.append({\n",
    "            'image_id' : d['image_hash'],\n",
    "            'caption' : c,\n",
    "            'id' : i\n",
    "        })\n",
    "        i += 1\n",
    "        \n",
    "result = {\n",
    "    'annotations' : annotations,\n",
    "    'images' : images,\n",
    "    'type' : 'captions', \n",
    "    'info' : 'dummy',\n",
    "    'licenses' : 'dummy'\n",
    "}\n",
    "\n",
    "with open('test_caption_coco_format.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-commission",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}